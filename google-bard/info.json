{
  "name": "google-bard",
  "version": "1.0.0",
  "label": "Google Bard",
  "description": "Google Bard is a conversational AI chatbot, based initially on the LaMDA family of large language models and later the PaLM LLM.",
  "publisher": "Fortinet",
  "cs_approved": true,
  "cs_compatible": true,
  "help_online": " https://docs.fortinet.com/document/fortisoar/1.0.0/google-bard/654/google-bard-v1-0-0",
  "icon_small_name": "small.png",
  "icon_large_name": "large.png",
  "category": "Miscellaneous",
  "configuration": {
    "fields": [
      {
        "title": "Server URL",
        "description": "Specify the URL of the Google Bard server to connect and perform automated operations.",
        "type": "text",
        "name": "server_url",
        "required": true,
        "editable": true,
        "visible": true,
        "value": "https://generativelanguage.googleapis.com",
        "tooltip": "Specify the URL of the Google Bard server to connect and perform automated operations."
      },
      {
        "title": "API Key",
        "description": "SSpecify the API key to access the endpoint to connect and perform the automated operations",
        "required": true,
        "editable": true,
        "visible": true,
        "type": "password",
        "name": "api_key",
        "tooltip": "Specify the API key to access the endpoint to connect and perform the automated operations"
      },
      {
        "title": "Verify SSL",
        "description": "Specifies whether the SSL certificate for the server is to be verified. By default, this option is set to True.",
        "name": "verify_ssl",
        "type": "checkbox",
        "required": false,
        "editable": true,
        "visible": true,
        "value": true,
        "tooltip": "Specifies whether the SSL certificate for the server is to be verified. By default, this option is set to True."
      }
    ]
  },
  "operations": [
    {
      "operation": "list_models",
      "title": "Get All Model List",
      "description": "Retrieves a list of all language models from Google Bard based on the pagination criteria and other parameters you have specified.",
      "category": "investigation",
      "annotation": "list_models",
      "enabled": true,
      "parameters": [
        {
          "title": "Page Size",
          "description": "(Optional) Specify the maximum count of records that this operation Page Size fetches from Google Bard. By default, this option is set to 50. You can specify a maximum value of 1000 and minimum value of 1.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "pageSize",
          "value": 50,
          "tooltip": "(Optional) Specify the maximum count of records that this operation Page Size fetches from Google Bard. By default, this option is set to 50. You can specify a maximum value of 1000 and minimum value of 1."
        },
        {
          "title": "Page Token",
          "description": "(Optional) Specify the token for the next set of items to return. The previously returned response contains a nextPageToken element containing a PageToken parameter. Use this parameter as a starting point to get the next page of results.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "pageToken",
          "tooltip": "(Optional) Specify the token for the next set of items to return. The previously returned response contains a nextPageToken element containing a PageToken parameter. Use this parameter as a starting point to get the next page of results."
        }
      ],
      "output_schema": {
        "models": [
          {
            "name": "",
            "topK": "",
            "topP": "",
            "version": "",
            "description": "",
            "displayName": "",
            "temperature": "",
            "inputTokenLimit": "",
            "outputTokenLimit": "",
            "supportedGenerationMethods": []
          }
        ],
        "nextPageToken": ""
      }
    },
    {
      "operation": "get_model_details",
      "title": "Get Model Details",
      "description": "Retrieves information for a specific model from Google Bard based on the model name you have specified.",
      "category": "investigation",
      "annotation": "get_model_details",
      "enabled": true,
      "parameters": [
        {
          "title": "Model Name",
          "description": "Specify the name of the model whose details are to be retrieved from Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "name",
          "placeholder": "For e.g. models/<model_name-version>"
        }
      ],
      "output_schema": {
        "name": "",
        "topK": "",
        "topP": "",
        "version": "",
        "description": "",
        "displayName": "",
        "temperature": "",
        "inputTokenLimit": "",
        "outputTokenLimit": "",
        "supportedGenerationMethods": []
      }
    },
    {
      "operation": "generate_text",
      "title": "Generate Text",
      "description": "Generates a response from the model based on the model name, text prompts, and other input parameters you have specified.",
      "category": "investigation",
      "annotation": "generate_text",
      "enabled": true,
      "parameters": [
        {
          "title": "Model Name",
          "description": "Specify the name of the model based on which to generate text in Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "name",
          "placeholder": "For e.g. models/<model_name-version>"
        },
        {
          "title": "Text Prompt",
          "description": "Specify a free-form input text given to the model as a prompt based on which Google Bard generates text.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "text",
          "tooltip": "Given a prompt, the model will generate a TextCompletion response it predicts as the completion of the input text."
        },
        {
          "title": "Safety Settings",
          "description": "(Optional) Specify a list of unique SafetySetting instances for blocking unsafe content.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "json",
          "name": "safetySettings",
          "tooltip": "(Optional) Note: There should not be more than one setting for each SafetyCategory type. It will block any prompts and responses that fail to meet the thresholds set by these settings. This list overrides the default settings for each SafetyCategory specified in the safetySettings. If there is no SafetySetting for a given SafetyCategory provided in the list, the API will use the default safety setting for that category. For e.g. [{\"category\": \"HARM_CATEGORY_DEROGATORY\", \"probability\": \"NEGLIGIBLE\"}]"
        },
        {
          "title": "Stop Sequences",
          "description": "(Optional) Specify a set of character sequences (up to 5) that stop the output generation.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "stopSequences",
          "tooltip": "(Optional) Note: If specified, it will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response."
        },
        {
          "title": "Temperature",
          "description": "(Optional) Specify the value of the temperature to control the randomness of the output. Values can range from [0.0,1.0], inclusive. A value closer to 1.0 produces responses that are more varied and creative, while a value closer to 0.0 typically results in a more straightforward responses from the model. NOTE: The default value varies by model.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "temperature",
          "placeholder": "For e.g. 0.0 to 1.0",
          "tooltip": "(Optional) Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model."
        },
        {
          "title": "Candidate Count",
          "description": "(Optional) Specify the candidate count based on which the number of generated responses to return. NOTE: This value must be between [1, 8], inclusive. If unset, defaults to 1.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "candidate_count",
          "value": 1,
          "placeholder": "For e.g. 1 to 8",
          "tooltip": "(Optional) Specify the candidate count based on which the number of generated responses to return. Note: This value must be between [1, 8], inclusive. If unset, this will default to 1."
        },
        {
          "title": "Maximum Output Tokens",
          "description": "(Optional) Specify the maximum number of tokens to include in a candidate. If unset, defaults to 64.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "maxOutputTokens",
          "value": 64,
          "tooltip": "(Optional) Specify the maximum number of tokens to include in a candidate. If unset, this will default to 64."
        },
        {
          "title": "Cumulative Probability (TopP)",
          "description": "(Optional) Specify the maximum cumulative probability of tokens to consider when sampling. NOTE: The default value varies by model, see the topP attribute of the Model returned by the Get Model Details response.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "topP",
          "tooltip": "(Optional) Model uses combined Top-k and nucleus sampling. Tokens are sorted based on their assigned probabilities so that only the most liekly tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability."
        },
        {
          "title": "Token (TopK)",
          "description": "(Optional) Specify the maximum number of tokens to consider when sampling. NOTE: The default value varies by model, see the topK attribute of the Model returned by the Get Model Details response.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "topK",
          "value": 40,
          "tooltip": "(Optional) Model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of topK most probable tokens. By default, it set to 40."
        }
      ],
      "output_schema": {
        "candidates": [
          {
            "output": "",
            "safetyRatings": [
              {
                "category": "",
                "probability": ""
              }
            ]
          }
        ]
      }
    },
    {
      "operation": "generate_embeddings",
      "title": "Generate Embedding",
      "description": "Generates an embedding from the model based on the model name and text you have specified.",
      "category": "investigation",
      "annotation": "generate_embeddings",
      "enabled": true,
      "parameters": [
        {
          "title": "Model Name",
          "description": "Specify the name of the model based on which to generate embedding in Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "name",
          "placeholder": "For e.g. models/<model_name-version>"
        },
        {
          "title": "Text",
          "description": "Specify the free-form input text that the model turns into an embedding in Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "text",
          "tooltip": "Specify the free-form input text that the model will turn into an embedding in Google Bard."
        }
      ],
      "output_schema": {
        "embedding": {
          "value": []
        }
      }
    },
    {
      "operation": "count_message_token",
      "title": "Count Message Token",
      "description": "Retrieves the number of tokens that the model creates from the prompt based on the model name, message, and other input parameter you have specified.",
      "category": "investigation",
      "annotation": "count_message_token",
      "enabled": true,
      "parameters": [
        {
          "title": "Model Name",
          "description": "Specify the name of the model based on which to retrieve token from Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "name",
          "placeholder": "For e.g. models/<model_name-version>"
        },
        {
          "title": "Messages",
          "description": "Specify the list of messages based on which to retrieve token count from Google Bard. NOTE: If the total input size exceeds the model's inputTokenLimit the input is truncated: The oldest items are dropped from the messages.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "json",
          "name": "messages",
          "tooltip": "Note: If the total input size exceeds the model's inputTokenLimit the input will be truncated: The oldest items will be dropped from messages. For e.g [{\"content\":\"hi\"}]"
        },
        {
          "title": "Context",
          "description": "(Optional) Specify the context of your prompt to the model to help provide context and guide the responses. If not empty, this context is given to the model first before the examples and messages. When using a context be sure to provide it with every request to maintain continuity.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "context",
          "tooltip": "(Optional) Specify the context can be a description of your prompt to the model to help provide context and guide the responses. Examples: \"Translate the phrase from English to French.\" or \"Given a statement, classify the sentiment as happy, sad or neutral. Note: Anything included in this field will take precedence over message history if the total input size exceeds the model's inputTokenLimit and the input request is truncated."
        },
        {
          "title": "Examples",
          "description": "(Optional) Specify a list of examples of what the model should generate in Google Bard. This includes both user input and the response that the model should emulate. NOTE: If the total input size exceeds the model's inputTokenLimit the input is truncated. Items are dropped from messages before examples.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "json",
          "name": "examples",
          "tooltip": "(Optional) Specify the list of examples of what the model should generate in Google Bard. This includes both user input and the response that the model should emulate. Note: If the total input size exceeds the model's inputTokenLimit the input will be truncated. Items will be dropped from messages before examples. For e.g [{\"input\": {\"author\": \"\", \"content\": \"\", \"citationMetadata\": {\"citationSources\": [{\"startIndex\": \"\",\"endIndex\": \"\", \"uri\": \"\",\"license\": \"\"}]}}]"
        }
      ],
      "output_schema": {
        "tokenCount": ""
      }
    },
    {
      "operation": "generate_message",
      "title": "Generate Message",
      "description": "Generates an message from the model based on the model name, messages, and other input parameters you have specified.",
      "category": "investigation",
      "annotation": "generate_message",
      "enabled": true,
      "parameters": [
        {
          "title": "Model Name",
          "description": "Specify the name of the model based on which to generate message in Google Bard.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "name",
          "placeholder": "For e.g. models/<model_name-version>"
        },
        {
          "title": "Messages",
          "description": "Specify the list of messages based on which to generate message in Google Bard. NOTE: If the total input size exceeds the model's inputTokenLimit the input is truncated: The oldest items are dropped from the messages.",
          "required": true,
          "editable": true,
          "visible": true,
          "type": "json",
          "name": "messages",
          "tooltip": "Note: If the total input size exceeds the model's inputTokenLimit the input will be truncated: The oldest items will be dropped from messages. For e.g [{\"content\":\"hi\"}]"
        },
        {
          "title": "Context",
          "description": "(Optional) Specify the context of your prompt to the model to help provide context and guide the responses. If not empty, this context is given to the model first before the examples and messages. When using a context be sure to provide it with every request to maintain continuity.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "context",
          "tooltip": "(Optional) Specify the context can be a description of your prompt to the model to help provide context and guide the responses. Examples: \"Translate the phrase from English to French.\" or \"Given a statement, classify the sentiment as happy, sad or neutral. Note: Anything included in this field will take precedence over message history if the total input size exceeds the model's inputTokenLimit and the input request is truncated."
        },
        {
          "title": "Examples",
          "description": "(Optional) Specify a list of examples of what the model should generate in Google Bard. This includes both user input and the response that the model should emulate. NOTE: If the total input size exceeds the model's inputTokenLimit the input is truncated. Items are dropped from messages before examples.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "json",
          "name": "examples",
          "tooltip": "(Optional) Specify the list of examples of what the model should generate message in Google Bard. This includes both user input and the response that the model should emulate. Note: If the total input size exceeds the model's inputTokenLimit the input will be truncated. Items will be dropped from messages before examples. For e.g [{\"input\": {\"author\": \"\", \"content\": \"\", \"citationMetadata\": {\"citationSources\": [{\"startIndex\": \"\",\"endIndex\": \"\", \"uri\": \"\",\"license\": \"\"}]}}]"
        },
        {
          "title": "Temperature",
          "description": "(Optional) Specify the value of the temperature to control the randomness of the output. Values can range from [0.0,1.0], inclusive. A value closer to 1.0 produces responses that are more varied and creative, while a value closer to 0.0 typically results in a more straightforward responses from the model. NOTE: The default value varies by model.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "temperature",
          "placeholder": "For e.g. 0.0 to 1.0",
          "tooltip": "(Optional) Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model."
        },
        {
          "title": "Candidate Count",
          "description": "(Optional) Specify the candidate count based on which the number of generated responses to return. NOTE: This value must be between [1, 8], inclusive. If unset, defaults to 1.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "candidate_count",
          "value": 1,
          "placeholder": "For e.g. 1 to 8",
          "tooltip": "(Optional) Specify the candidate count based on which the number of generated responses to return. Note: This value must be between [1, 8], inclusive. If unset, this will default to 1."
        },
        {
          "title": "Cumulative Probability (TopP)",
          "description": "(Optional) Specify the maximum cumulative probability of tokens to consider when sampling. NOTE: The default value varies by model, see the topP attribute of the Model returned by the Get Model Details response.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "text",
          "name": "topP",
          "tooltip": "(Optional) Model uses combined Top-k and nucleus sampling. Tokens are sorted based on their assigned probabilities so that only the most liekly tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability."
        },
        {
          "title": "Token (TopK)",
          "description": "(Optional) Specify the maximum number of tokens to consider when sampling. NOTE: The default value varies by model, see the topK attribute of the Model returned by the Get Model Details response.",
          "required": false,
          "editable": true,
          "visible": true,
          "type": "integer",
          "name": "topK",
          "value": 40,
          "tooltip": "(Optional) Model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of topK most probable tokens. By default, it set to 40."
        }
      ],
      "output_schema": {
        "messages": [
          {
            "author": "",
            "content": ""
          }
        ],
        "candidates": [
          {
            "author": "",
            "content": ""
          }
        ]
      }
    }
  ]
}